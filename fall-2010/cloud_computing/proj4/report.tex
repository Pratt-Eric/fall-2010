Testing for existing master in group: simon
Creating group simon-master
GROUP	simon-master	Group for Hadoop Master.
GROUP		simon-master	
PERMISSION		simon-master	ALLOWS	all     FROM	USER	869345430376  GRPNAME	simon-master
GROUP		simon-master	
PERMISSION		simon-master	ALLOWS	tcp	22	22	FROM	CIDR	0.0.0.0/0
GROUP		simon-master	
PERMISSION		simon-master	ALLOWS	tcp	50030	50030	FROM	CIDR	0.0.0.0/0
GROUP		simon-master	
PERMISSION		simon-master	ALLOWS	tcp	50060	50060	FROM	CIDR	0.0.0.0/0
GROUP		simon-master	
PERMISSION		simon-master	ALLOWS	tcp	50070	50070	FROM	CIDR	0.0.0.0/0
GROUP		simon-master	
PERMISSION		simon-master	ALLOWS	tcp	50075	50075	FROM	CIDR	0.0.0.0/0
Starting master with AMI ami-fa6a8e93
Waiting for instance i-56d1723b to start
................................Started as ip-10-126-38-236.ec2.internal
Warning: Permanently added 'ec2-204-236-244-156.compute-1.amazonaws.com,204.236.244.156' (RSA) to the list of known hosts.
Copying private key to master
id_rsa-gsg-keypair                                                                   100\% 1671     1.6KB/s   00:00    
Master is ec2-204-236-244-156.compute-1.amazonaws.com, ip is 204.236.244.156, zone is us-east-1b.
Adding simon node(s) to cluster group simon with AMI ami-fa6a8e93
i-90d675fd
i-92d675ff
i-6cd57601
i-6ed57603
i-68d57605
i-6ad57607
i-64d57609
i-66d5760b
i-60d5760d
i-62d5760f

(1) How do you pass a script file to an EC2 instance, when you start
up an instance?

Judging from the following description, we can know that scp is used
to pass a script file to an EC2 instance. 

push <group> <file> scp a file to the master node of the Hadoop EC2
cluster

(2) After an instance is started, how to pass a command or a script
and execute it in the instance?

Following command option is used with hadoop-ec2 script to pass a
command or a script and execute it in the instance. 
<shell cmd> <group|instance id> execute any command remotely on
the master

(3) Check the file hadoop-ec2-init-remote.sh and describe what the
script does for master node and data node, respectively.

On the master node, it first configure and startup the ganglia
monitoring service, then namenode are formated(once), and finally,
namenode and jobtracker are started. 

On the slave nodes, ganglia monitoring service is configured and
started. Then hadoop datanode and tasktracker are started. 

(1) does the master node work normally in the single node mode? Are
the processes: namenode, jobtracker and secondary namenode started
normally? 

No, the master node doesn't work normally in the single node
mode. Although I can check the process of namenode and the
jobtracker. When I try to run the example map-reduce program using
hadoop, it doesn't work at all. 
And I can test the system with jps and get following output:
1429 Jps
1167 NameNode
1213 JobTracker

which means there is no datanode running. 

When I try to run the stop-all.sh script under hadoop/bin
installation, the prompt tells me that no namenode, jobtracker,
datanode to stop. 

Then, I tried to start the single node cluster by using the
start-all.sh script, and the example program can be run. 
We can also use the jps command to show the status of hadoop
system. For example, after successfully setup the hadoop system, we
can use this command to display the status as below: 
2782 SecondaryNameNode
2677 DataNode
2876 JobTracker
4158 Jps
2587 NameNode
2978 TaskTracker

(2) can the data nodes communicate with the master node with keyless
ssh? If not, how do you change the HadoopEC2 script? 

No, public key is required for ssh to work without a key. 

(3) are the processes: datanode and tasktracker in the data node
started normally? If not, what is the reason? You may check whether
all the ports are correctly authorized. 

On multinode mode, the datanode on the master node won't start because
of a bug related to hadoop. I followed the workaround by change the
namespaceID of the data node (the file on my system is
/mnt/hadoop/dfs/data/current/VERSION)

When setting up a two node cluster, I can't access the master node
from the slave node, the reason I found is that the hostname and the
configuration in the /etc/hosts doesn't match. The workaround is to
change the hostname of slave to the name of configuration in the
/etc/hosts file or add the current hostname to the /etc/hosts. 